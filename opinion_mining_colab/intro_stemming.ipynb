{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming con nltk\n",
    "\n",
    "El kit de herramientas de lenguaje natural (NLTK), es un conjunto de bibliotecas y programas para el procesamiento del lenguaje natural (PLN) para el lenguaje de programación Python. \n",
    "\n",
    "Tareas básicas:\n",
    "\n",
    "1. Proceso de tokenización: distinguir los tokens (palabras individuales, signos de puntuación entre otros) y eliminar las palabras vacías (stopwords)\n",
    "2. Stemming: Extraer la raíz de cada palabra. Por ejemplo, familia, familiar, familias se reduce a famili\n",
    "\n",
    "Para realizar el stemming se utiliza el algoritmo de Porter, que es un algoritmo de derivación de palabras y consiste en eliminar terminaciones morfológicas de las palabras en inglés/español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0imFU6W0j-1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\raul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '¿', '¡', 'AT_USER', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import nltk.classify\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "#import re\n",
    "\n",
    "# Si no tenemos los stopwords hay que descargarlos:\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Para textos en español se utiliza el algoritmo de Porter2\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "non_words = list(punctuation)\n",
    "\n",
    "# agregamos algunos signos de puntuación del español\n",
    "non_words.extend(['¿', '¡','AT_USER'])\n",
    "non_words.extend(map(str, range(10)))\n",
    "print(non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1520444685772,
     "user": {
      "displayName": "Raul Oramas",
      "photoUrl": "//lh3.googleusercontent.com/-qhJxdQev4gI/AAAAAAAAAAI/AAAAAAAAAfA/DYA88iohB30/s50-c-k-no/photo.jpg",
      "userId": "112881898687901104051"
     },
     "user_tz": 420
    },
    "id": "BiGtEey_j-1b",
    "outputId": "2d6bb139-f6b3-49e8-a888-affc65019e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aprend\n",
      "aprend\n",
      "famili\n",
      "famili\n",
      "famili\n",
      "['som', 'campeon', '!', '!', '!', 'brillant', 'victori', '!', '!', 'gan', '...', '1250', 'lik', ':', '-', ')', 'yuhuuuu', '!', '!', '|']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 1\n",
    "\n",
    "print(stemmer.stem('aprender'))\n",
    "print(stemmer.stem('aprendiendo'))\n",
    "\n",
    "print(stemmer.stem('familia'))\n",
    "print(stemmer.stem('familiar'))\n",
    "print(stemmer.stem('familias'))\n",
    "\n",
    "text = 'Somos campeones!!! Brillante victoria!! Ganamos... 1250 likes :-) yuhuuuu!!|'\n",
    "stemmed_text = [stemmer.stem(i) for i in word_tokenize(text)]\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1520444687722,
     "user": {
      "displayName": "Raul Oramas",
      "photoUrl": "//lh3.googleusercontent.com/-qhJxdQev4gI/AAAAAAAAAAI/AAAAAAAAAfA/DYA88iohB30/s50-c-k-no/photo.jpg",
      "userId": "112881898687901104051"
     },
     "user_tz": 420
    },
    "id": "kTzNNwEAj-1g",
    "outputId": "2c07e9cf-dbd1-4300-cca0-7ecc1afe0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Si', ',', 'si', ',', 'siii', ',', 'Somos', 'campeones', '!', '!', '!', ',', 'Raúl', 'GANAMOS', '...', '1250+', ':', '-', ')', '#', 'yuhuuuu', ',', ',', ',', '@', 'kbza', 'hola', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 2\n",
    "\n",
    "text = 'Si, si, siii, Somos campeones!!!, Raúl GANAMOS... 1250+ :-)   #yuhuuuu,,,  @kbza hola.'\n",
    "output = word_tokenize(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2511,
     "status": "ok",
     "timestamp": 1520444691873,
     "user": {
      "displayName": "Raul Oramas",
      "photoUrl": "//lh3.googleusercontent.com/-qhJxdQev4gI/AAAAAAAAAAI/AAAAAAAAAfA/DYA88iohB30/s50-c-k-no/photo.jpg",
      "userId": "112881898687901104051"
     },
     "user_tz": 420
    },
    "id": "hldkv_mPj-1j",
    "outputId": "14fc3761-08fc-47f9-e17c-ad22953081ff"
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens,stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4rTOlKMBj-1l"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uHPXxP_Bj-1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['si', 'si', 'siii', 'som', 'campeon', 'raul', 'gan', 'yuhuuuu', 'kbza', 'hol']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 3\n",
    "text = 'Si, si, siii, Somos campeones!!!, Raúl GANAMOS... 1250+ :-)   #yuhuuuu,,,  @kbza hola.'\n",
    "output = tokenize(text)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "index.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
